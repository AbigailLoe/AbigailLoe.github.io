---
layout: paper
title: "Counterfactually Fair Reinforcement Learning via Sequential Data Preprocessing"
image: /assets/images/papers/cfrl.png
authors: Jitao Wang, Chengchun Shi, John Piette, Joshua Loftus, Donglin Zeng, Zhenke Wu
year: submitted
shortref: Wang et al.
journal: 
pdf:
slides:
supplement:
poster: /assets/pdfs/posters/2025-03-27 poster_CFRL.pdf
github:
doi:
external_link: https://arxiv.org/abs/2501.06366
video_link: 
type: statistical
tags:
    - mobile health
    - reinforcement learning
    - ethical AI
    - fairness
 
---

# Abstract

When applied in healthcare, reinforcement learning (RL) seeks to dynamically match the right interventions to subjects to maximize population benefit. However, the learned policy may disproportionately allocate efficacious actions to one subpopulation, creating or exacerbating disparities in other socioeconomically-disadvantaged subgroups. These biases tend to occur in multi-stage decision making and can be self-perpetuating, which if unaccounted for could cause serious unintended consequences that limit access to care or treatment benefit. Counterfactual fairness (CF) offers a promising statistical tool grounded in causal inference to formulate and study fairness. In this paper, we propose a general framework for fair sequential decision making. We theoretically characterize the optimal CF policy and prove its stationarity, which greatly simplifies the search for optimal CF policies by leveraging existing RL algorithms. The theory also motivates a sequential data preprocessing algorithm to achieve CF decision making under an additive noise assumption. We prove and then validate our policy learning approach in controlling unfairness and attaining optimal value through simulations. Analysis of a digital health dataset designed to reduce opioid misuse shows that our proposal greatly enhances fair access to counseling.

**Keywords**: Counterfactual Fairness, Digital Health, Opioid Misuse, Reinforcement Learning, Sequential Data Preprocessing.